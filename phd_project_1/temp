intelligent code review assignment for large scale open source software stacks ishan aryendu stevens institute of technology hoboken, nj, usa iaryendu@stevens.eduying wang stevens institute of technology hoboken, nj, usa ywang6@stevens.edu farah elkourdi stevens institute of technology hoboken, nj, usa felkourd@stevens.edueman alomar stevens institute of technology hoboken, nj, usa ealomar@stevens.edu abstract in the process of developing software, code review is crucial. by identifying problems before they arise in production, it enhances the quality of the code. finding the best reviewer for a code change, however, is extremely challenging especially in large scale, espe- cially open source software stacks with cross functioning designs and collaborations among multiple developers and teams. addition- ally, a review by someone who lacks knowledge and understanding of the code can result in high resource consumption and technical errors. the reviewers who have the specialty in both functioning (domain knowledge) and non-functioning areas of a commit are con- sidered as the most qualified reviewer to look over any changes to the code. quality attributes serve as the connection among the user requirements, delivered function description, software architecture and implementation through put the entire software stack cycle. in this study, we target on auto reviewer assignment in large scale software stacks and aim to build a self-learning, and self-correct platform for intelligently matching between a commit based on its quality attributes and the skills sets of reviewers. to achieve this, quality attributes are classified and abstracted from the commit messages and based on which, the commits are assigned to the reviewers with the capability in reviewing the target commits. we first designed machine learning schemes for abstracting quality attributes based on historical data from the openstack repository. two models are built and trained for automating the classifica- tion of the commits based on their quality attributes using the manual labeling of commits and multi-class classifiers. we then positioned the reviewers based on their historical data and the quality attributes characteristics. finally we selected the recom- mended reviewer based on the distance between a commit and candidate reviewers. in this paper, we demonstrate how the models can choose the best quality attributes and assign the code review to the most qualified reviewers. with a comparatively small training permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than acm must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. ase ’22, october 10–14, 2022, rochester, mi, usa ©2022 association for computing machinery. acm isbn 978-1-4503-9475-8/22/10. . . $15.00 https://doi.org/10.1145/3551349.3561147dataset, the models are able to achieve f-1 scores of 77% and 85.31%, respectively. keywords large-scale, open-source, code review, commit classification, machine learning, mpnet acm reference format: ishan aryendu, ying wang, farah elkourdi, and eman alomar. 2022. in- telligent code review assignment for large scale open source software stacks. in 37th ieee/acm international conference on automated software engineering (ase ’22), october 10–14, 2022, rochester, mi, usa. acm, new york, ny, usa, 6 pages. https://doi.org/10.1145/3551349.3561147 1 introduction quality attributes are the non-functional requirements evaluated by the developers and stakeholders. software architects extract quality attributes from requirements specification to enhance the architecture design, raise stakeholders satisfaction and apply them into the software design and implementations. identifying code check-ins related to quality attribute assist in architecture design and improving the software quality. maintenance is an anticipated phase in the project life cycle. this phase might include project expansion or bug fixes related to certain quality attribute(s) such as security enhancement, performance improvement, test coverage or improve test-ability and documentation related. review and confirmation during the maintenance phase are also required to fulfill any required changes. improper assignments of the reviews results in inefficiency of the software development and increase the risks of faults and vulnerabilities in them, especially large scale software systems. in this paper, we will use machine learning techniques to automate the assignment process for devel- opers and architects based on the history of the reviewers and the quality attribute(s) extracted from the developer’s commit messages. the goal of this study is to streamline the review allocation process using the traits of quality attributes. when choosing reviewers based on the application fields, the reviewers with software exper- tise are constrained because the functional features of software can vary significantly depending on the application fields.the primary factor determining how well and consistently software performs over time is its quality attributes. allocating reviewers based on quality attributes results in more efficient task allocation among reviewers and improves software quality in the long run.ase ’22, october 10–14, 2022, rochester, mi, usa ishan aryendu, ying wang, farah elkourdi, and eman alomar as code check-ins include modifications or new functionalities, the system architecture may be impacted. each code check-in can be associated with one or more quality attributes. as a result, archi- tects will be able to define a more accurate architectural structure in which code check-in modifications are taken into account and linked to expected quality attributes. researchers have used several approaches for code reviewer recommendation. we have studied a variety of research papers [ 8]. numerous methods for automating source code reviewers’ recommendations have been developed over time [ 1,6,11,14,18]. based on the primary features they take into account and the methods they adopt to suggest code reviewers, we summarized the most pertinent existing recommendation al- gorithms into four groups: i) heuristic, ii) machine learning, iii) social networks, and iv) hybrid approaches. to identify the most pertinent code reviewers, traditional rec- ommendation approaches analyze data from previous project re- views and employ heuristic-based algorithms. the three main algo- rithms are code reviewer recommendation based on cross-project and technology experience (correct) [ 11], reviewbot[ 1], and revfinder[ 14][7]. assumptions for factors that associating the exist- ing reviewed commits and to-be reviewed commits in the category of heuristic algorithms include: expertise assumption, familiar- ity assumption, and file location. for ’expertise assumption’, in the correct system [ 11], the underlying principle is that the reviewers of the prior pull request are also suitable candidates for the current one if the previous pull request used a similar exter- nal library or technology. for ’familiarity assumption’, a method called reviewbot has been put forth by balachandran [ 1]. it is a code reviewer recommendation method based primarily on the idea that lines of code modified in the pull request ought to be reviewed by the same code reviewers who previously discussed or modified those lines of code. for ’file location assumption’, revfinder[ 14] is based on the location of the files that are part of pull requests. certain number of points given to the code reviewers who previ- ously reviewed the files increases in proportion to how similar the file paths are. machine learning based algorithms recommend code reviewers using a variety of machine learning approaches. they differ from the previous group primarily because they must first construct a model based on training data. the authors of "predicting reviewers and acceptance of patches"[ 6] employ the bayesian network technique to forecast reviewers and patch acceptance based on a number of features, including patch meta-data, patch content, and bug report details. social networks have also been used to identify communication patterns among developers, resulting in the recommendation of candidates for source code reviews who are more likely to be similar. by examining the social connections between contributors and developers, [ 18] proposed the comment network (cn) approach as a method for recommending code reviewers. the foundation of cn-based recommendations is the notion that it is possible to infer developers’ interests from their interactions with comments. as mentioned in [ 18], appropriate code reviewers are developers who have similar interests to the person who created a pull request. in hybrid approaches, the group of algorithms combines various techniques (machine learning, social network analysis) to suggest code reviewers. the work in [ 7] is an illustration of this approach.in the past, sentence embeddings have been the subject of in- depth research. each approach has a unique approach to solving the problem. for instance, skip-thought [ 9] uses an encoder-decoder architecture to determine how the sentences around it relate to one another. while infersent [ 3] makes use of a max-pooling layer and a siamese bilstm network. even though the skipthought method is effective, infersent consistently outperforms it [ 3]. the universal sentence encoder [ 2] combines unsupervised training with snli training to train a transformer network. on the sts benchmark dataset, a combination of siamese dan and siamese transformer networks produced good results [17]. adapting the deployment scenario is a significant challenge when deploying the current code review assignment algorithms. the assumptions in the use scenario must be met for the heuristic al- gorithms to be accurate. a wide range of algorithms are covered by machine learning techniques. to achieve the anticipated accuracy, the training dataset must also meet certain requirements. accuracy is precisely described as the sum of the true positives and true nega- tives divided by the sum of the true positives and true negatives, as well as the true positives and true negatives, respectively. however, it may be deceptive and constrained by privacy concerns. so we use another measure for comparison by calculating the harmonic mean of a classifier’s precision and recall and combining them both into a single metric, the f-1 score. it mainly serves to compare the effectiveness of two classifiers. social networks offer strong pivots for reviewers selection with the rich information in their profiles, networks, and activities. for many conditions, hybrid approaches that combine the benefits of the models described above are used more frequently. due to their complex architecture and the diversity of the developer and reviewer community, large scale open source systems pose the greatest challenge among use cases and existing techniques. the challenges couldn’t be fully addressed by any of the earlier algo- rithms. another obstacle is the need for non-functional quality attributes like security, modifiability, and ongoing development, where reviewers’ experience and knowledge are crucial to the soft- ware stack’s quality. certain software stacks function as vital infras- tructures. for example, in 5th generation cellular communications software defined radio platforms like open air interface (oai) [ 10] and srsran [4]. thus in this paper, we presented an innovative code review assignment system that consider both functioning and non- functioning features of a commit for reviewer allocation. the pro- posed system addressed the challenges in large scale open source software stack. the main contribution of this study includes: (1)a proposed quality-attribute based intelligent code-review assignment system that addressed the challenges in large- scale open-source software stacks. the study integrated both supervised learning-based and unsupervised learning-based algorithms with self-learning capability to improve the learn- ing accuracy over the time. (2)customized nlp models and transform learning scheme are developed for quality attributes abstraction from the commit message. both commercial platform and in-house developed models are explored and compared. a modified version of the masked and permuted language modeling (mpnet) isintelligent code review assignment for large scale open source software stacks ase ’22, october 10–14, 2022, rochester, mi, usa designed and implemented aiming to determine the quality attributes for a specific commit message. (3)umap-based algorithm is used for identifying relative- position of the classification results of commit messages. the shortest euclidean distance between the calculated point and the centroid point for each user is then used to characterize the reviewers specialty and assign the target commits to the matching reviewers. (4)the proposed system provides enhancement for the coher- ence in development process, and significantly reduces the development and maintainers cost through out the product life cycle. it allows an intelligent and efficient tasks alloca- tion, which leads to improvement in code quality and reduce potential risks due to improper reviewing process. the rest of the paper is organized as follows. we first introduced the backgrounds and basic components that serves as the founda- tion of the proposed system in section 2. we then described our proposed system in section 3, followed by the system performance assessment at section 4. finally, the conclusion and future work is presented in section 5. 2 backgrounds and related work in this section, we go over the fundamentals of intelligent code re- view and automatic reviewer assignments. we first describe natural language processing (nlp) keyword abstraction scheme exploring existing commercial nlp platform. we then discuss models that use transformers in nlp. to build in-house customizable classification model, we have developed transfer learning model based on a model named mpnet [ 13]. at last, we presented the clustering methods used for reviewer matching to build our proposed systems. monkey-learn is a user-friendly artificial intelligence platform that offers a ready-to-train text analysis model without any coding skills. this platform is based on supervised learning which requires user annotation. due to its many cost-effective pricing options for product teams and developers and high quality performance, monkey-learn is widely used in the industry [ ?]. in this study, we have utilized the text analyzer’s free trial plan, which offers 1,000 training data and 1,000 outputs each month and compared with our in-house classification model. transformer models are useful in transfer learning from a large- scale lm (language model), which have been pre-trained on a significant amount of text for conceptual tasks, such as sentiment analysis and even predictive analysis. a transformer is a deep learn- ing model that uses the self-attention process and weights the importance of each component of the input data differently [ 15]. the goal of transformer architecture is to account for the distant relationships between words with ease while solving sequential problems. in contrast, alternative models which are available for creating information-rich representations of sentences and para- graphs are sentence transformers. these models are trained using a number of techniques, but the unsupervised pre-training of a trans- former model using techniques like masked-language modeling is the first step.we use mpnet for transfer learning. in order to make the most of the dependencies between predicted tokens, mpnet employs per- muted language modeling. it also uses auxiliary position informa- tion as input to help the model recognize complete sentences, which lowers position discrepancy. the primary advantage of mpnet over other transformer based models is that it uses more information when predicting a masked token, resulting in better representa- tions while learning and less discrepancy with the downstream tasks. since bert [ 5] ignores the relationships between predicted tokens, xlnet [ 16] introduces permuted language modeling (plm) for pre-training to address this issue. however, xlnet suffers from position discrepancy between pre-training and fine-tuning because it does not fully exploit the position information contained in a sentence. according to experimental findings, mpnet significantly outperforms mlm and plm on these tasks and outperforms earlier state-of-the-art pre-trained methods (such as bert, xlnet, and roberta) when used in the same model setting [ 13]. mpnet inher- its the benefits of bert and xlnet while avoiding their drawbacks. 3 system description the objective of the proposed system is establish an self-learning, and self-correct platform for intelligently code review assignments to match between a commit based on its quality attributes and the skills sets of reviewers. the primary targeting software stacks are large scale open source software stacks where the modifia- bility, reliability and security significantly influence the quality of software stack or delivered system. the challenges of code re- view assignment in such systems include cross functioning designs and collaborations among multiple developers or teams, which require the reviewers’ expertise in both domain knowledge and non-functioning software architectures. quality attributes serve as the connection among the user requirements, delivered func- tion description, software architecture and implementation through put the entire software stack cycle. thus, in our proposed system, quality attributes are classified and abstracted from the commit messages and based on which, the commits are assigned to the reviewers with the capability in reviewing the target commits. in fig.1, we have described the overall system design and imple- mentation, including multiple independent modules and interfaces bridging the modules. we start by selecting openstack as our pre- ferred software stack. next, we divide the collected data samples into commits and reviewer histories. preprocessing is done on the data obtained from the commit samples for training and testing. the monkeylearn and the mpnet models receive the annotated data samples as input. for every commit, these models predict the quality attribute and for dimensionality reduction, the output is fed to the umap model along with the commit reviewer data. the plot containing the centroid coordinates for the top reviewers and each commit sample is created using the 2-d points of the output. after that, we determine the euclidean distance to compare the reviewers to the commits. in order to assign the reviewers in an intelligent manner, we take into consideration external factors like domain expertise, team members, etc.ase ’22, october 10–14, 2022, rochester, mi, usa ishan aryendu, ying wang, farah elkourdi, and eman alomar figure 1: system architecture overview 3.1 data collection the code review process of the openstack is based on gerrit, collab- orative code review framework allowing developers to directly tag submitted code changes and request its assignment to a reviewer. generally, a code change author opens a code review request con- taining a title, a detailed description of the code change being submitted, written in natural language, along with the current code changes annotated. once the review request is submitted, it appears in the requests backlog, open for reviewers to choose. once review- ers are assigned to the review request, they inspect the proposed changes and comment on the review request’s thread, to start a discussion with the author. we mined code review data using the restful api1provided by gerrit, which returns the results in a json format. we used a script to automatically mine the review data and store them in sqlite database. all collected reviews are closed (i.e., having a status of either ‘ merged ‘ or ‘ abandoned ’). 3.2 experiment setup and annotation first, we established the tags of annotation on both monkey-learn platform and our models. the following tags have been used: doc- umentation related, performance improvement, test coverage or improve test-ability, security enhancement, improve availability, improve usability, achieving interoperability, improving modifia- bility.we began the training process after identifying the tags. with 1,000 check-in data, we trained the model with manually assigned quality attribute(s). monkey-learn tracked our behavior to identify a pattern that can be used to categorize the data going forward. the model carried out this process by identifying words that are related to the same tagged quality attribute in the classified check-ins. the model is now ready to receive an input, process it, and produce an output. based on the identified patterns during the training process, monkey-learn will produce a list of words associated with each tag. the input text will be automatically analyzed by the model and assigned one or more tags. the model has phases for building, training, and running. dupli- cation is removed from the data during the preparation phase. no ambiguous commits were removed, and no typos or grammatical 1https://gerrit-review.googlesource.com/documentation/rest-apichanges.htmlerrors were fixed. eight tags were defined, and the text may fit into one or more of these groups: documentation related, perfor- mance improvement, test coverage or improve test-ability, security enhancement, improve availability, improve usability, achieving interoperability, and improving modifiability. we uploaded 1,000 commits for training purpose. the trained data includes the subject and check-in code descriptions, which are required to extract the quality attribute. to create a classification model, the monkey-learn model is fed training data that consists of pairs of code-check-ins (title and description) and tags/labels (security, availability, usabil- ity, etc.). during the training phase, each of the 1,000 commits is assigned one or more labels. the machine learning model can begin to make accurate predictions once it has been trained with enough training samples and will determine which tags are associated with each input. but it is imperative to use data other than the trained data to accurately test whether the model is working properly. in fig.2, we find that most of the data were related to documentation, improve availability and testing. in fig.3, a few tags were having less training data which made it more challenging to have correct output related to them while testing. several commits included a description indicating that development bugs were fixed. those commits were skipped and did not contribute to the 1,000 tagged data. we were unable to assign them to a specific quality attribute. the result shows overall statistics for monkey learn model with an accuracy of 71% and f1 score of 77%. our code reviewer recommendation model, using historical code reviews, examines how frequently the recommended review- ers were the actual reviewers for each quality attribute. how- ever, we are going to assume that the reviewer assignment in the past was correct. the hyper-parameters used in the model includes: maxlength =512,trainingbatchsize = 32,validationbatchsize = 32,epochs = 40, learningrate =1e−05. 4 system performance we get an accuracy of 79.291% and an f-1 score of 85.31% when we use the mpnet-based model to make predictions. the accuracy rises to 84.89% when the first two tags predicted by the model are considered, and to 87.5% when the first three tags predicted by the model are considered. for each of the quality attributes, theintelligent code review assignment for large scale open source software stacks ase ’22, october 10–14, 2022, rochester, mi, usa figure 2: texts percentage per each tag for 1k data figure 3: number of texts per each tag model determines the confidence levels. the values of the prediction vector and the value of the quality attribute for each row are chosen in order to further visualize the results. these values are then fed into the umap as shown in fig.4 respectively, which produce the 2-d coordinates for the commit messages. an 8-d vector is reduced to a 2-d vector in the process. we discovered that the umap algorithm produces better clustering of our data when we compare the visualizations. as a result, we assign reviewers based on the coordinates generated by the umap clustering. the centroid points in fig.5 are then calculated for the top twenty reviewers who are not part of a team of multiple reviewers. this makes it easier for us to assign commits to reviewers for review. the euclidean distance is used to predict which commit should be assigned to those reviewers. the distribution is depicted in fig.6 figure 4: umap clustering figure 5: centroid for the users figure 6: number of predicted commits 5 conclusion to address the issues with large-scale open-source software stacks, we proposed an intelligent code-review assignment system that is based on quality attributes. for increasing the learning ac- curacy over time, the proposed platform combined supervised learning-based and unsupervised learning-based algorithms with self-learning capabilities. a modified version of the mpnet model was created and put into practice with the intention of identifying the qualities of a given commit message. after that, a umap-based algorithm was used to determine the relative position of the commit message with respect to the classification results. the reviewers’ ex- pertise are then predicted and the target commits are assigned to the matching reviewers using the shortest euclidean distance between the calculated point and the centroid point for each reviewer. at the end of this study, we found that the two models per- formed admirably at classifying code commits according to their quality attributes. with the data that has been given to it, the pro- prietary monkey-learn model functions well, but it has a number of restrictions. for instance, it was unable to provide us with the probabilities for each of the eight quality attributes. because of this, it was difficult to determine the two-dimensional coordinates for the commits. the mpnet-based model, however, performed better in terms of accuracy and flexibility. it could provide us with the eight-dimensional vector values to calculate the coordinates for reviewer assignment, and was 11.7% more accurate than the propri- etary solution. when considering the f1 score, the mpnet-based model outperformed the monkey-learn model by a margin of 10.8%. 6 future work in this paper, we show that the mpnet-based transformer model can predict the reviewers who will be able to effectively addressase ’22, october 10–14, 2022, rochester, mi, usa ishan aryendu, ying wang, farah elkourdi, and eman alomar the pull request and can account for the complex interactive pat- terns between the entities in the code review ecosystem. while the data from the openstack code repository shows promise, we think that by training the model on the data unique to any other repository will be useful in making recommendations. the model can be used by any project to streamline their workflow because it is quite generic and can be trained on any dataset made available to it. here, we primarily focused on the relationship between the commit messages in the openstack code repository and the quality attributes. the files and directory where the changes were made were not taken into consideration. the model may learn intricate patterns in the data by keeping track of those features, increasing the accuracy of selecting the reviewers. additionally, we think that a thorough examination of the hyperparameters would result in greater accuracy. in our future work, we want to examine the effect of these parameters on the overall behavior of the model. references [1]vipin balachandran. 2013. reducing human effort and improving quality in peer code reviews using automatic static analysis and reviewer recommendation. in proceedings - international conference on software engineering . https://doi.org/ 10.1109/icse.2013.6606642 [2]daniel m. cer, mona t. diab, eneko agirre, iñigo lopez-gazpio, and lucia specia. 2017. semeval-2017 task 1: semantic textual similarity - multilingual and cross- lingual focused evaluation. corr abs/1708.00055 (2017). arxiv:1708.00055 http://arxiv.org/abs/1708.00055 [3]alexis conneau and douwe kiela. 2018. senteval: an evaluation toolkit for uni- versal sentence representations. corr abs/1803.05449 (2018). arxiv:1803.05449 http://arxiv.org/abs/1803.05449 [4]dimitri dessources, stephen mayhew, ying wang, daniel setareh, jeff reed, and shehadi dayekh. 2022. fuzz testing for 4g/5g open stack protocol security enhancement. in ieee international conference on network softwarization . [5]jacob devlin, ming-wei chang, kenton lee, and kristina toutanova. 2018. bert: pre-training of deep bidirectional transformers for language understanding. corr abs/1810.04805 (2018). arxiv:1810.04805 http://arxiv.org/abs/1810.04805 [6]gaeul jeong, sunghun kim, and thomas zimmermann. 2009. improving code review by predicting reviewers and acceptance of patches. research on software (2009). [7]jing jiang, jia huan he, and xue yuan chen. 2015. coredevrec: automatic core member recommendation for contribution evaluation. journal of computer science and technology 30, 5 (2015). https://doi.org/10.1007/s11390-015-1577-3 [8]jiyang zhang, chandra maddil, ram bairi, christian bird, ujjwal raizada, apoorva agrawal, yamini jhawar, kim herzig, and arie van deursen. 2022. using large-scale heterogeneous graph representationlearning for code re- view recommendations. in arxiv:2202.02385v2 , vol. 2657. ceur-ws, 1–9. https: //doi.org/10.1145/nnnnnnn.nnnnnnn [9]ryan kiros, yukun zhu, ruslan salakhutdinov, richard s. zemel, antonio tor- ralba, raquel urtasun, and sanja fidler. 2015. skip-thought vectors. in advances in neural information processing systems , vol. 2015-january. [10] sergio martiradonna, alessandro grassi, giuseppe piro, and gennaro boggia. 2020. 5g-air-simulator: an open-source tool modeling the 5g air interface. computer networks 173 (5 2020). https://doi.org/10.1016/j.comnet.2020.107151 [11] mohammad masudur rahman, chanchal k. roy, and jason a. collins. 2016. correct: code reviewer recommendation in github based on cross-project and technology experience. in proceedings - international conference on software engineering . https://doi.org/10.1145/2889160.2889244 [12] ]ren-etal-2021-autolibrary yichun ren, jiayi fan, and bingqi zhou. [n. d.]. au- tolibrary - a personal digital library to find related works via text analyzer. https://dsc-capstone.github.io/projects-2020-2021/reports/project_39.pdf [13] kaitao song, xu tan, tao qin, jianfeng lu, and tie-yan liu. 2020. mp- net: masked and permuted pre-training for language understanding. in advances in neural information processing systems , h. larochelle, m. ran- zato, r. hadsell, m.f. balcan, and h. lin (eds.), vol. 33. curran asso- ciates, inc., 16857–16867. https://proceedings.neurips.cc/paper/2020/file/ c3a690be93aa602ee2dc0ccab5b7b67e-paper.pdf [14] patanamon thongtanunam, chakkrit tantithamthavorn, raula gaikovina kula, norihiro yoshida, hajimu iida, and ken ichi matsumoto. 2015. who should review my code? a file location-based code-reviewer recommendation approach for modern code review. in 2015 ieee 22nd international conference on software analysis, evolution, and reengineering, saner 2015 - proceedings . https://doi. org/10.1109/saner.2015.7081824[15] thomas wolf, lysandre debut, victor sanh, julien chaumond, clement delangue, anthony moi, pierric cistac, tim rault, remi louf, morgan funtowicz, joe davison, sam shleifer, patrick von platen, clara ma, yacine jernite, julien plu, canwen xu, teven le scao, sylvain gugger, mariama drame, quentin lhoest, and alexander rush. 2020. transformers: state-of-the-art natural language processing. in proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations . association for computational linguistics, online, 38–45. https://doi.org/10.18653/v1/2020.emnlp-demos.6 [16] ping yang, yue xiao, ming xiao, and shaoqian li. 2019. 6g wireless com- munications: vision and potential techniques. ieee network 33, 4 (2019). https://doi.org/10.1109/mnet.2019.1800418 [17] yinfei yang, steve yuan, daniel cer, sheng-yi kong, noah constant, petr pilar, heming ge, yun-hsuan sung, brian strope, and ray kurzweil. 2018. learning semantic textual similarity from conversations. corr abs/1804.07754 (2018). arxiv:1804.07754 http://arxiv.org/abs/1804.07754 [18] yue yu, huaimin wang, gang yin, and tao wang. 2016. reviewer recom- mendation for pull-requests in github: what can we learn from code re- view and bug assignment? information and software technology 74 (2016). https://doi.org/10.1016/j.infsof.2016.01.004
